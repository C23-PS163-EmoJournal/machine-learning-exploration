{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-13T20:32:34.667508Z","iopub.execute_input":"2023-06-13T20:32:34.667905Z","iopub.status.idle":"2023-06-13T20:32:34.676732Z","shell.execute_reply.started":"2023-06-13T20:32:34.667851Z","shell.execute_reply":"2023-06-13T20:32:34.675492Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nCREMA = \"/kaggle/input/cremad/AudioWAV/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:36.854597Z","iopub.execute_input":"2023-06-13T20:32:36.854971Z","iopub.status.idle":"2023-06-13T20:32:36.864619Z","shell.execute_reply.started":"2023-06-13T20:32:36.854913Z","shell.execute_reply":"2023-06-13T20:32:36.863483Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['JE_h09.wav', 'KL_f12.wav', 'DC_h03.wav', 'DC_d04.wav', 'KL_a14.wav']"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"savee\"></a>\n##  <center> 1. SAVEE dataset <center>\nThe audio files are named in such a way that the prefix letters describes the emotion classes as follows:\n- 'a' = 'anger'\n- 'd' = 'disgust'\n- 'f' = 'fear'\n- 'h' = 'happiness'\n- 'n' = 'neutral'\n- 'sa' = 'sadness'\n- 'su' = 'surprise' \n\nThe original source has 4 folders each representing a speaker, but i've bundled all of them into one single folder and thus the first 2 letter prefix of the filename represents the speaker initials. Eg. 'DC_d03.wav' is the 3rd disgust sentence uttered by the speaker DC. It's  worth nothing that they are all male speakers only. This isn't an issue as we'll balance it out with the TESS dataset which is just female only. So lets check out the distribution of the emotions...","metadata":{}},{"cell_type":"markdown","source":"<a id=\"savee_load\"></a>\n###  Load the dataset \nI'm not going to be reading the entire audio to memory. Rather I'm just going to read the meta-data associated with it. Cause at this point I just want a high level snapshot of some statistics. And then I might just load 1 or 2 audio files and expand on it. \n\nSo lets take 2 different emotions and play it just to get a feel for what we are dealing with. Ie. whether the data (audio) quality is good. It gives us an early insight as to how likely our classifier is going to be successful.  ","metadata":{}},{"cell_type":"code","source":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n    \n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:45.420617Z","iopub.execute_input":"2023-06-13T20:32:45.420981Z","iopub.status.idle":"2023-06-13T20:32:45.457941Z","shell.execute_reply.started":"2023-06-13T20:32:45.420927Z","shell.execute_reply":"2023-06-13T20:32:45.456699Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"male_neutral     120\nmale_surprise     60\nmale_disgust      60\nmale_sad          60\nmale_angry        60\nmale_happy        60\nmale_fear         60\nName: labels, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"ravdess\"></a>\n## <center>2. RAVDESS dataset</center>\n\nRAVDESS is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. And there's more! You can get it in song format as well. There's something for everyone and their research project. So for convenience, here's the filename identifiers as per the official RAVDESS website:\n\n- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n- Vocal channel (01 = speech, 02 = song).\n- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. \n_02-01-06-01-02-01-12.mp4_\n\nThis means the meta data for the audio file is:\n- Video-only (02)\n- Speech (01)\n- Fearful (06)\n- Normal intensity (01)\n- Statement \"dogs\" (02)\n- 1st Repetition (01)\n- 12th Actor (12) - Female (as the actor ID number is even)\n\nAt my early beginings embarking on this journey, I learnt through the hard way that male and female speakers have to be trained seperately or the model will struggle to get a good accuracy. From reading a few blogs and articles, it seems female has a higher pitch that male. So if we don't tag the gender label to the audio file, it won't be able to detect anger or fear if it was a male speaker. It will just get bucketed into neutral \n\nLets specifically model the 2 speakers seperately. Note that there's a 'calm' emotion and a 'neutral' emotion as seperate. I don't really know the difference but for now, I'll just combined them into the same category.","metadata":{}},{"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:47.781123Z","iopub.execute_input":"2023-06-13T20:32:47.781532Z","iopub.status.idle":"2023-06-13T20:32:48.263090Z","shell.execute_reply.started":"2023-06-13T20:32:47.781463Z","shell.execute_reply":"2023-06-13T20:32:48.262098Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"male_neutral       144\nfemale_neutral     144\nfemale_angry        96\nfemale_happy        96\nmale_surprise       96\nfemale_surprise     96\nfemale_sad          96\nmale_disgust        96\nfemale_disgust      96\nmale_fear           96\nmale_sad            96\nfemale_fear         96\nmale_angry          96\nmale_happy          96\nName: labels, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"tess\"></a>\n##  <center> 3. TESS dataset <center>\nNow on to the TESS dataset, its worth nothing that it's only based on 2 speakers, a young female and an older female. This should hopefully balance out the male dominant speakers that we have on SAVEE. \n\nIts got the same 7 key emotions we're interested in. But what is slightly different about this dataset compared to the previous two above, is that the addition of 'pleasant surprise' emotion. I haven't really checked to see for the RADVESS and SAVEE dataset, if the surpises are unpleasant. But I'm going to work with the assumption for now that its a pleasant surprise. If we find out from post modelling, surpise is highly inaccurate, we can come back and modify our assumption here. ","metadata":{}},{"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:50.209074Z","iopub.execute_input":"2023-06-13T20:32:50.209501Z","iopub.status.idle":"2023-06-13T20:32:50.221291Z","shell.execute_reply.started":"2023-06-13T20:32:50.209432Z","shell.execute_reply":"2023-06-13T20:32:50.220237Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['OAF_Fear',\n 'OAF_Pleasant_surprise',\n 'OAF_Sad',\n 'OAF_angry',\n 'OAF_disgust',\n 'OAF_happy',\n 'OAF_neutral',\n 'YAF_angry',\n 'YAF_disgust',\n 'YAF_fear',\n 'YAF_happy',\n 'YAF_neutral',\n 'YAF_pleasant_surprised',\n 'YAF_sad']"},"metadata":{}}]},{"cell_type":"code","source":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:51.422283Z","iopub.execute_input":"2023-06-13T20:32:51.422702Z","iopub.status.idle":"2023-06-13T20:32:52.096257Z","shell.execute_reply.started":"2023-06-13T20:32:51.422628Z","shell.execute_reply":"2023-06-13T20:32:52.095185Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"female_surprise    400\nfemale_fear        400\nfemale_angry       400\nfemale_happy       400\nfemale_sad         400\nfemale_disgust     400\nfemale_neutral     400\nName: labels, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"crema\"></a>\n##  <center> 4. CREMA-D dataset <center>\nLast but not least, CREMA dataset. Not much is known about this dataset and I don't see much usage of this in general in the wild. But its a very large dataset which we need. And it has a good variety of different speakers, apparently taken from movies. And the speakers are of different ethnicities. This is good. Means better generalisation when we do transfer learning. Very important\n\nWhat we are missing from this dataset is the \"surprise\" emotion but no biggie, we can use the rest. But we have the rest. What's extra here is that it has different level of intensity of the emotion like RAVDESS. But we won't be using that for now","metadata":{}},{"cell_type":"code","source":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:56.462345Z","iopub.execute_input":"2023-06-13T20:32:56.462706Z","iopub.status.idle":"2023-06-13T20:32:56.744352Z","shell.execute_reply.started":"2023-06-13T20:32:56.462656Z","shell.execute_reply":"2023-06-13T20:32:56.743097Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['1001_DFA_ANG_XX.wav', '1001_DFA_DIS_XX.wav', '1001_DFA_FEA_XX.wav', '1001_DFA_HAP_XX.wav', '1001_DFA_NEU_XX.wav', '1001_DFA_SAD_XX.wav', '1001_IEO_ANG_HI.wav', '1001_IEO_ANG_LO.wav', '1001_IEO_ANG_MD.wav', '1001_IEO_DIS_HI.wav']\n","output_type":"stream"}]},{"cell_type":"code","source":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:32:57.540967Z","iopub.execute_input":"2023-06-13T20:32:57.541391Z","iopub.status.idle":"2023-06-13T20:32:57.609158Z","shell.execute_reply.started":"2023-06-13T20:32:57.541316Z","shell.execute_reply":"2023-06-13T20:32:57.608118Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"male_angry        671\nmale_happy        671\nmale_fear         671\nmale_sad          671\nmale_disgust      671\nfemale_angry      600\nfemale_happy      600\nfemale_fear       600\nfemale_sad        600\nfemale_disgust    600\nmale_neutral      575\nfemale_neutral    512\nName: labels, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## 5. Final","metadata":{}},{"cell_type":"code","source":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"df_male_female_7emo.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:33:01.819278Z","iopub.execute_input":"2023-06-13T20:33:01.819670Z","iopub.status.idle":"2023-06-13T20:33:02.348462Z","shell.execute_reply.started":"2023-06-13T20:33:01.819602Z","shell.execute_reply":"2023-06-13T20:33:02.347277Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"female_angry       1096\nfemale_happy       1096\nfemale_fear        1096\nfemale_sad         1096\nfemale_disgust     1096\nfemale_neutral     1056\nmale_neutral        839\nmale_disgust        827\nmale_angry          827\nmale_happy          827\nmale_fear           827\nmale_sad            827\nfemale_surprise     496\nmale_surprise       156\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:33:04.564469Z","iopub.execute_input":"2023-06-13T20:33:04.564826Z","iopub.status.idle":"2023-06-13T20:33:04.589568Z","shell.execute_reply.started":"2023-06-13T20:33:04.564776Z","shell.execute_reply":"2023-06-13T20:33:04.588471Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         labels source                                               path\n0    male_happy  SAVEE  /kaggle/input/surrey-audiovisual-expressed-emo...\n1     male_fear  SAVEE  /kaggle/input/surrey-audiovisual-expressed-emo...\n2    male_happy  SAVEE  /kaggle/input/surrey-audiovisual-expressed-emo...\n3  male_disgust  SAVEE  /kaggle/input/surrey-audiovisual-expressed-emo...\n4    male_angry  SAVEE  /kaggle/input/surrey-audiovisual-expressed-emo...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>source</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>male_happy</td>\n      <td>SAVEE</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>male_fear</td>\n      <td>SAVEE</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>male_happy</td>\n      <td>SAVEE</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>male_disgust</td>\n      <td>SAVEE</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>male_angry</td>\n      <td>SAVEE</td>\n      <td>/kaggle/input/surrey-audiovisual-expressed-emo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}